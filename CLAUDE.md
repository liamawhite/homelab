# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Development Commands

The project uses Nix for development environment management, Go for CLI/infrastructure, and Yarn for TypeScript package management.

**Go CLI Commands:**
- `go run cli/main.go bootstrap --node <name>` - Bootstrap a Raspberry Pi node
- `go run cli/main.go k3s --node <name> --cluster-init` - Initialize K3s cluster
- `go run cli/main.go k3s --node <name> --server <url>` - Join node to cluster
- `go run cli/main.go kubeconfig --node <name>` - Extract kubeconfig
- `go run cli/main.go clustertoken --node <name>` - Extract cluster token

**Pulumi Go Commands:**
- `cd pulumi && pulumi preview` - Preview infrastructure changes
- `cd pulumi && pulumi up` - Deploy kube-vip and core components
- `cd pulumi && pulumi destroy` - Destroy Pulumi-managed resources

**TypeScript Development Commands:**
- `nix develop` - Enter the development shell with all required tools
- `yarn check` - Run code generation and formatting (equivalent to `yarn gen && yarn format`)
- `yarn gen` - Generate Kubernetes CRD TypeScript bindings from upstream sources
- `yarn format` - Format all code using Prettier
- `yarn deploy` - Deploy the homelab infrastructure using Pulumi (runs `yarn homeassistant` first)
- `yarn homeassistant` - Extract Home Assistant configuration from running pod
- `yarn passwords` - Extract and display secrets from the Pulumi stack
- `yarn k9s` - Launch k9s Kubernetes dashboard using the generated kubeconfig

All commands should be run within the Nix development shell (`nix develop`) which provides:

- crd2pulumi (for generating TypeScript from Kubernetes CRDs)
- kubernetes-helm, k9s, jq, yq, git, nodejs_24, go, docker, gnumake

## Project Architecture

This is a hybrid Infrastructure as Code project that deploys a complete homelab on Raspberry Pi hardware using:
- **Go CLI** (`cli/`) - Bootstraps infrastructure (Raspberry Pi provisioning, K3s installation)
- **Pulumi Go** (`pulumi/`) - Deploys core Kubernetes components (kube-vip for control plane HA)
- **Pulumi TypeScript** (root/`project/`) - Legacy deployment for applications (being gradually migrated to Go)

### Go CLI and Infrastructure (`cli/`, `pulumi/`, `pkg/`)

The Go-based tooling handles infrastructure bootstrapping and core Kubernetes components:

1. **CLI Tool** (`cli/`):
   - `cmd/bootstrap.go` - Raspberry Pi provisioning (SSH keys, system updates)
   - `cmd/k3s.go` - K3s installation (cluster init or join)
   - `cmd/kubeconfig.go` - Kubeconfig extraction
   - `cmd/clustertoken.go` - Cluster token extraction (saves to `./cluster-token`)
   - `pkg/k3s/` - K3s installer implementation
   - `pkg/ssh/` - SSH client for remote operations

2. **Pulumi Go** (`pulumi/`):
   - `main.go` - Entry point, orchestrates deployment
   - `config.go` - Configuration loading (merges `infra.yaml` + Pulumi config)
   - `provider.go` - Kubernetes provider from `kubeconfig`
   - `pkg/kubevip/` - Kube-vip component for control plane HA
     - `component.go` - ComponentResource implementation
     - `rbac.go` - ServiceAccount, ClusterRole, ClusterRoleBinding
     - `daemonset.go` - DaemonSet with kube-vip configuration

3. **Shared Config** (`pkg/config/`):
   - `config.go` - Shared configuration structure used by both CLI and Pulumi
   - Loads from `infra.yaml` at repository root
   - Handles precedence: CLI flags > infra.yaml > env vars > defaults

4. **Configuration Files**:
   - `infra.yaml` - Main configuration (VIP, nodes, SSH settings, kube-vip version)
   - `kubeconfig` - Generated by CLI, used by Pulumi (gitignored)
   - `cluster-token` - Generated by `clustertoken` command (gitignored)

**Workflow**:
```bash
# 1. Bootstrap first node
go run cli/main.go bootstrap --node pi-0
go run cli/main.go k3s --node pi-0 --cluster-init
go run cli/main.go kubeconfig --node pi-0
go run cli/main.go clustertoken --node pi-0  # Saves to ./cluster-token

# 2. Deploy kube-vip via Pulumi
cd pulumi && pulumi up

# 3. Join additional nodes via VIP
go run cli/main.go bootstrap --node pi-1
go run cli/main.go k3s --node pi-1 --server https://192.168.1.50:6443
```

### Legacy TypeScript Infrastructure Stack (being migrated)

1. **Hardware Layer** (`components/infra/`):

    - `raspberrypi5/` - Raspberry Pi 5 configuration and provisioning
    - `k3s/` - Lightweight Kubernetes distribution setup
    - `pki/` - Public Key Infrastructure for certificates

2. **Networking & VPN** (`project/vpn.ts`, `project/network.ts`):

    - Tailscale integration for secure remote access
    - MetalLB for LoadBalancer services
    - DNS configuration with Pi-hole

3. **Kubernetes Applications** (`components/kubernetes/`):
    - `certmanager/` - Automatic TLS certificate management
    - `istio/` - Service mesh with Gateway API
    - `metallb/` - Bare metal load balancer
    - `pihole/` - Network-wide ad blocking and DNS
    - `tailscale/` - VPN operator for Kubernetes
    - `externaldns/` - Automatic DNS record management
    - `homeassistant/` - Home automation platform
    - `prometheus-operator/` - Monitoring and alerting
    - `grafana/` - Visualization and dashboards
    - `node-exporter/` - Node-level metrics collection
    - `kube-state-metrics/` - Kubernetes object metrics
    - `cadvisor/` - Container-level metrics via kubelet
    - `longhorn/` - Distributed block storage

### Key Configuration

- **Main entry point**: `project/index.ts` orchestrates the entire deployment with dependency order (VPN → Cluster → PKI → Storage → Network → DNS → Monitoring → Applications)
- **Configuration**: `project/config.ts` loads node connection details and Tailscale credentials from Pulumi config
- **Version management**: `.versions.ts` centralizes all application versions and uses current Git commit for custom builds
- **CRD Generation**: `.gen.ts` script orchestrates TypeScript binding generation for all Kubernetes operators

### Generated Files

**Go Infrastructure:**
- `kubeconfig` - Generated by CLI at repo root for kubectl/k9s access (gitignored)
- `cluster-token` - Generated by `clustertoken` command at repo root (gitignored)
- `ca.pem` - Root CA certificate for the PKI (gitignored)

**TypeScript Infrastructure:**
- `components/kubernetes/*/crds/` - Generated TypeScript CRD bindings from upstream sources
- `components/kubernetes/*/gen/` - CRD generation scripts that fetch and process upstream CRDs

### Component Architecture

Each Kubernetes component follows a consistent pattern:

1. **Component class** (`index.ts`) - Main component with configuration and dependencies
2. **CRD generation script** (`gen/*.ts`) - Fetches upstream CRDs and generates TypeScript bindings
3. **Generated CRDs** (`crds/`) - TypeScript definitions for Kubernetes Custom Resources
4. **Version pinning** - All versions centralized in `.versions.ts` for consistency

### Monitoring Stack

- **Prometheus Operator** - Manages Prometheus instances and monitoring configuration
- **ServiceMonitors** - Automatic service discovery for metrics scraping
- **Grafana Dashboards** - Located in `project/monitoring/dashboards/` as JSON files
- **Cadvisor Component** - Provides container-level metrics via kubelet ServiceMonitor
- **Dashboard Management** - Dashboards exported via `project/monitoring/dashboards/index.ts`

### Development Workflow

1. Infrastructure changes are made in TypeScript using Pulumi
2. Run `yarn gen` to regenerate any CRD bindings if upstream charts change
3. Use `yarn format` to ensure consistent code style
4. Deploy with `yarn deploy` (requires proper Pulumi configuration)
5. Access cluster with `yarn k9s` or standard kubectl with the generated kubeconfig

The project follows a modular component architecture where each service is self-contained but can depend on shared infrastructure like PKI and networking.

### Important Implementation Details

**Go Infrastructure:**
- **Kube-vip Configuration**: Control plane HA only (`svc_enable=false`), MetalLB handles LoadBalancer services
- **Network Interface**: Hardcoded to `eth0` (Raspberry Pi default)
- **VIP Address**: 192.168.1.50 (configured in `infra.yaml`)
- **Leader Election**: Uses Kubernetes leases for automatic failover
- **Node Affinity**: Only runs on control plane nodes (`node-role.kubernetes.io/control-plane=true`)
- **Security**: Requires NET_ADMIN and NET_RAW capabilities for ARP

**TypeScript Infrastructure:**
- **Pulumi Force Patching**: Deployment uses `PULUMI_K8S_ENABLE_PATCH_FORCE=true` to handle immutable field updates
- **Git-based Versioning**: Custom components use current Git commit SHA for versioning (see `.versions.ts`)
- **CRD Lifecycle**: CRDs are regenerated from upstream sources, not manually maintained
- **Dependency Management**: Services have explicit dependencies enforced through Pulumi resource relationships
- **Resource Sizing**: Prometheus requires adequate memory (2Gi limit) for container metrics collection
- **Grafana Dashboards**: Use kube-state-metrics `exported_*` labels for actual pod information, not scraper pod labels

## Troubleshooting

### Go Infrastructure

**Kube-vip pods not running:**
```bash
kubectl get pods -n kube-system -l app.kubernetes.io/name=kube-vip
kubectl logs -n kube-system -l app.kubernetes.io/name=kube-vip
```
Common issues:
- Interface not found: Verify `eth0` exists on your Raspberry Pi
- VIP already in use: Ensure 192.168.1.50 is not assigned to another device
- No control plane nodes: Requires nodes with `node-role.kubernetes.io/control-plane=true`

**VIP not responding:**
```bash
ping 192.168.1.50
kubectl get leases -n kube-system | grep vip
```

**Cluster token not found:**
Ensure you've run `go run cli/main.go clustertoken --node pi-0` which saves to `./cluster-token`

**SSH connection fails:**
Set password via environment variable: `export HOMELAB_SSH_PASSWORD=<password>`

### TypeScript Infrastructure

**`no IP addresses available in range set`:**
`ssh` into the node and run `sudo rm -rf /var/lib/cni/networks/cbr0 && sudo reboot`. See [k3s issue](https://github.com/k3s-io/k3s/issues/4682).

**k3s node fails:**
Check `systemctl status k3s.service` and `journalctl -xeu k3s.service` for details.

**Home Assistant Configuration:**
The `yarn deploy` command automatically extracts Home Assistant configuration before deployment using `components/kubernetes/homeassistant/extract-config.ts`. This processes the YAML configuration files in `components/kubernetes/homeassistant/config/` and makes them available to the deployment.
